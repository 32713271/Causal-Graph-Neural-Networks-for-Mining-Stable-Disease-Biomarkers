{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing, model building, and evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from scipy.stats import pointbiserialr\n",
    "from math import sqrt\n",
    "import copy\n",
    "import math\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "from scipy.stats import chi2\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "def data_get():\n",
    "    disese = pd.read_csv(r\"D:\\jupyter\\New\\dateset\\mRNA_LncRNA_Non_Small_Cell_Lung_Carcinoma.csv\")\n",
    "    health = pd.read_csv(r\"D:\\jupyter\\New\\dateset\\mRNA_LncRNA_Non_Small_Cell_Lung_Carcinoma_HC.csv\")\n",
    "    \n",
    "    # 提取“蛋白编码”基因的数据\n",
    "    dat = disese[disese['gene_type'] == \"protein_coding\"]\n",
    "    tes = health[health['gene_type'] == \"protein_coding\"]\n",
    "    column_name = dat.iloc[:, 2].values  # 获取基因名称\n",
    "    # 获取样本的数量：数据的列是样本，行是基因\n",
    "    dat_samples = dat.iloc[:, 6:].T  # 只取样本数据，转置后每行是一个样本\n",
    "    tes_samples = tes.iloc[:, 6:].T  # 同样对健康数据处理，转置后每行是一个样本\n",
    "    \n",
    "    return dat_samples, tes_samples, column_name\n",
    "\n",
    "# 使用修改后的数据加载函数\n",
    "dat_ace, tes_ace, column_name = data_get()\n",
    "\n",
    "# 合并数据\n",
    "df_data = pd.concat([dat_ace, tes_ace], axis=0)  # 沿着行（样本）合并数据\n",
    "\n",
    "# 假设标签在第二列\n",
    "df_data.columns = [column_name]\n",
    "\n",
    "# 标签处理：疾病为1，健康为0\n",
    "dis_lab = [1] * len(dat_ace)  # 疾病标签\n",
    "HC_lab = [0] * len(tes_ace)  # 健康标签\n",
    "label = dis_lab + HC_lab  # 合并标签\n",
    "\n",
    "df_Z = scale(df_data, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "expr_data = pd.DataFrame(df_Z, columns=df_data.columns.get_level_values(0).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad3f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'D:\\jupyter\\new_funtion\\data\\data1\\o_adata.csv')\n",
    "df_data = data.drop(columns='Label')\n",
    "label = data['Label']\n",
    "df_Z = preprocessing.scale(df_data, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "expr_data = pd.DataFrame(df_Z,columns=df_data.columns.get_level_values(0).values, index=df_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载基因间关系数据\n",
    "interaction_data_path = r'D:\\jupyter\\new_funtion\\2024_7_9\\download.txt'\n",
    "interaction_data = pd.read_csv(interaction_data_path, sep=\"\\t\", header=None)\n",
    "interaction_data.columns = ['raid', 'actor1', 'cat1', 'sp1', 'actor2', 'cat2', 'sp2', 'score']\n",
    "\n",
    "# 准备基因交互网络（图）\n",
    "genes = expr_data.columns  # 基因列\n",
    "adj_matrix = lil_matrix((len(genes), len(genes)))  # 创建空的邻接矩阵\n",
    "\n",
    "# 创建基因到索引的映射\n",
    "gene_to_idx = {gene: idx for idx, gene in enumerate(genes)}\n",
    "\n",
    "# 填充邻接矩阵\n",
    "for index, row in interaction_data.iterrows():\n",
    "    if row['actor1'] in gene_to_idx and row['actor2'] in gene_to_idx:\n",
    "        i, j = gene_to_idx[row['actor1']], gene_to_idx[row['actor2']]\n",
    "        adj_matrix[i, j] = 1\n",
    "        adj_matrix[j, i] = 1\n",
    "\n",
    "# 转换为CSR格式以提高计算效率\n",
    "adj_matrix_csr = adj_matrix.tocsr()\n",
    "adj_matrix_coo = coo_matrix(adj_matrix_csr)  # 转换为COO格式用于边的索引\n",
    "\n",
    "# 准备图神经网络的边索引\n",
    "edge_index = torch.tensor([adj_matrix_coo.row, adj_matrix_coo.col], dtype=torch.long)\n",
    "\n",
    "# 准备特征矩阵（基因表达数据）\n",
    "x = torch.tensor(expr_data.values.T, dtype=torch.float)  # 特征应该对应到节点\n",
    "data = Data(x=x, edge_index=edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536964c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition:\n",
    "# `ImprovedGCN` is a Graph Convolutional Network model with multiple convolution layers.\n",
    "# The model learns both node features and topological graph structure through the graph convolution layers.\n",
    "# It includes dropout and batch normalization to improve performance.\n",
    "# 加载基因间关系数据\n",
    "interaction_data_path = r'D:\\jupyter\\new_funtion\\2024_7_9\\download.txt'\n",
    "interaction_data = pd.read_csv(interaction_data_path, sep=\"\\t\", header=None)\n",
    "interaction_data.columns = ['raid', 'actor1', 'cat1', 'sp1', 'actor2', 'cat2', 'sp2', 'score']\n",
    "\n",
    "# 准备基因交互网络（图）\n",
    "genes = expr_data.columns  # 基因列\n",
    "adj_matrix = lil_matrix((len(genes), len(genes)))  # 创建空的邻接矩阵\n",
    "\n",
    "# 创建基因到索引的映射\n",
    "gene_to_idx = {gene: idx for idx, gene in enumerate(genes)}\n",
    "\n",
    "# 填充邻接矩阵\n",
    "for index, row in interaction_data.iterrows():\n",
    "    if row['actor1'] in gene_to_idx and row['actor2'] in gene_to_idx:\n",
    "        i, j = gene_to_idx[row['actor1']], gene_to_idx[row['actor2']]\n",
    "        adj_matrix[i, j] = 1\n",
    "        adj_matrix[j, i] = 1\n",
    "\n",
    "# 转换为CSR格式以提高计算效率\n",
    "adj_matrix_csr = adj_matrix.tocsr()\n",
    "adj_matrix_coo = coo_matrix(adj_matrix_csr)  # 转换为COO格式用于边的索引\n",
    "\n",
    "# 准备图神经网络的边索引\n",
    "edge_index = torch.tensor([adj_matrix_coo.row, adj_matrix_coo.col], dtype=torch.long)\n",
    "\n",
    "# 准备特征矩阵（基因表达数据）\n",
    "x = torch.tensor(expr_data.values.T, dtype=torch.float)  # 特征应该对应到节点\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "features = torch.tensor(expr_data.values.T, dtype=torch.float)  # 注意转置以便每列特征对应一个节点\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ImprovedGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels=64):\n",
    "        super(ImprovedGCN, self).__init__()\n",
    "        # 添加多层GCN和残差连接\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, num_node_features)\n",
    "        \n",
    "        # 添加批归一化\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # 添加dropout防止过拟合\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # 第一层\n",
    "        identity = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 第二层\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 第三层\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # 添加残差连接\n",
    "        return x + identity\n",
    "\n",
    "class ImprovedGraphLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        super(ImprovedGraphLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, predictions, targets, edge_index):\n",
    "        # MSE损失\n",
    "        mse_loss = F.mse_loss(predictions, targets)\n",
    "        \n",
    "        # 添加拓扑结构保持损失\n",
    "        row, col = edge_index\n",
    "        edge_loss = torch.mean(torch.abs(\n",
    "            torch.norm(predictions[row] - predictions[col], dim=1) -\n",
    "            torch.norm(targets[row] - targets[col], dim=1)\n",
    "        ))\n",
    "        \n",
    "        return mse_loss + self.alpha * edge_loss\n",
    "\n",
    "# 模型初始化\n",
    "input_dim = features.shape[1]\n",
    "model = ImprovedGCN(input_dim)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "criterion = ImprovedGraphLoss()\n",
    "\n",
    "# 训练过程\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data)\n",
    "    loss = criterion(out, data.x, data.edge_index)\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 学习率调整\n",
    "    scheduler.step(loss)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# 使用最佳模型进行预测\n",
    "model = best_model\n",
    "out = model(data)\n",
    "\n",
    "# 将模型输出转换为DataFrame格式\n",
    "out_np = out.detach().cpu().numpy()\n",
    "data_gcn = pd.DataFrame(out_np.T, columns=[f'Feature_{i}' for i in range(out_np.shape[0])], index=expr_data.index)\n",
    "data_gcn.columns = expr_data.columns.get_level_values(0).values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition:\n",
    "# `ImprovedGCN` is a Graph Convolutional Network model with multiple convolution layers.\n",
    "# The model learns both node features and topological graph structure through the graph convolution layers.\n",
    "# It includes dropout and batch normalization to improve performance.\n",
    "features = torch.tensor(expr_data.values.T, dtype=torch.float)  # 注意转置以便每列特征对应一个节点\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ImprovedGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels=64):\n",
    "        super(ImprovedGCN, self).__init__()\n",
    "        # 添加多层GCN和残差连接\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, num_node_features)\n",
    "        \n",
    "        # 添加批归一化\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # 添加dropout防止过拟合\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # 第一层\n",
    "        identity = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 第二层\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 第三层\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # 添加残差连接\n",
    "        return x + identity\n",
    "\n",
    "class ImprovedGraphLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        super(ImprovedGraphLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, predictions, targets, edge_index):\n",
    "        # MSE损失\n",
    "        mse_loss = F.mse_loss(predictions, targets)\n",
    "        \n",
    "        # 添加拓扑结构保持损失\n",
    "        row, col = edge_index\n",
    "        edge_loss = torch.mean(torch.abs(\n",
    "            torch.norm(predictions[row] - predictions[col], dim=1) -\n",
    "            torch.norm(targets[row] - targets[col], dim=1)\n",
    "        ))\n",
    "        \n",
    "        return mse_loss + self.alpha * edge_loss\n",
    "\n",
    "# 模型初始化\n",
    "input_dim = features.shape[1]\n",
    "model = ImprovedGCN(input_dim)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "criterion = ImprovedGraphLoss()\n",
    "\n",
    "# 训练过程\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data)\n",
    "    loss = criterion(out, data.x, data.edge_index)\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 学习率调整\n",
    "    scheduler.step(loss)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# 使用最佳模型进行预测\n",
    "model = best_model\n",
    "out = model(data)\n",
    "\n",
    "# 将模型输出转换为DataFrame格式\n",
    "out_np = out.detach().cpu().numpy()\n",
    "data_gcn = pd.DataFrame(out_np.T, columns=[f'Feature_{i}' for i in range(out_np.shape[0])], index=expr_data.index)\n",
    "data_gcn.columns = expr_data.columns.get_level_values(0).values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing, model building, and evaluation\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 假设 data_gcn 和 expr_data 已经存在，且具有相同的形状和列名称（基因名）\n",
    "\n",
    "# 删除重复列名（确保每列唯一）\n",
    "data_gcn = data_gcn.loc[:, ~data_gcn.columns.duplicated(keep='first')]\n",
    "expr_data = expr_data.loc[:, ~expr_data.columns.duplicated(keep='first')]\n",
    "\n",
    "# 创建一个 DataFrame 来存储每个基因的皮尔森相关系数\n",
    "pearson_scores = []\n",
    "\n",
    "# 填充缺失值为 0（根据实际需求调整）\n",
    "data_gcn = data_gcn.fillna(0)\n",
    "expr_data = expr_data.fillna(0)\n",
    "\n",
    "for gene in data_gcn.columns:\n",
    "    try:\n",
    "        # 提取当前基因的值（确保是 Series）\n",
    "        gcn_values = data_gcn[gene].squeeze()\n",
    "        original_values = expr_data[gene].squeeze()\n",
    "        \n",
    "        # 转换为数值型数据（处理可能的非数值类型）\n",
    "        gcn_values = pd.to_numeric(gcn_values, errors='coerce')\n",
    "        original_values = pd.to_numeric(original_values, errors='coerce')\n",
    "        \n",
    "        # 删除含有 NaN 的行\n",
    "        valid_data = pd.DataFrame({\n",
    "            'gcn': gcn_values,\n",
    "            'original': original_values\n",
    "        }).dropna()\n",
    "        \n",
    "        if valid_data.empty:\n",
    "            raise ValueError(f\"No valid data for gene {gene} after dropping NaN.\")\n",
    "        \n",
    "        # 计算皮尔森相关系数\n",
    "        correlation, _ = pearsonr(valid_data['gcn'], valid_data['original'])\n",
    "        pearson_scores.append({\"Gene\": gene, \"Pearson_Correlation\": correlation})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing gene {gene}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 转换为 DataFrame\n",
    "pearson_df = pd.DataFrame(pearson_scores)\n",
    "\n",
    "# 筛选出皮尔森系数在 -1 到 -0.8 或 0.8 到 1 之间的基因\n",
    "filtered_genes = pearson_df[\n",
    "    (pearson_df[\"Pearson_Correlation\"] >= 0.7) | \n",
    "    (pearson_df[\"Pearson_Correlation\"] <= -0.7)\n",
    "]\n",
    "\n",
    "# 提取符合条件的基因列表\n",
    "selected_genes = filtered_genes[\"Gene\"].tolist()\n",
    "\n",
    "# 从 data_gcn 和 expr_data 中提取对应基因的数据\n",
    "selected_data_gcn = data_gcn[selected_genes]\n",
    "selected_expr_data = expr_data[selected_genes]\n",
    "\n",
    "# 打印结果\n",
    "print(\"Selected Genes and Pearson Correlation Coefficients:\")\n",
    "print(pearson_df)\n",
    "print(\"\\nData from data_gcn:\")\n",
    "print(selected_data_gcn.head())\n",
    "print(\"\\nData from expr_data:\")\n",
    "print(selected_expr_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fc749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing, model building, and evaluation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error  # Import mean_squared_error\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 累计ACE的总和\n",
    "total_ace = 0  \n",
    "# 用于存储ACE值\n",
    "ace_values = []\n",
    "\n",
    "# 假设expr_data, data_gcn已经定义且格式正确\n",
    "# expr_data: 基因表达数据\n",
    "# data_gcn: GCN模型输出数据\n",
    "# feature_columns = expr_data.columns.get_level_values(0).values \n",
    "feature_columns = filtered_genes[\"Gene\"]  # 替换为正确的列名\n",
    "\n",
    "# 遍历每一个特征列\n",
    "for column_name in feature_columns:\n",
    "    try:\n",
    "        # 确保列存在\n",
    "        if column_name not in data_gcn or column_name not in expr_data:\n",
    "            print(f\"Column {column_name} not found in one of the datasets.\")\n",
    "            continue\n",
    "\n",
    "        f_s_array = data_gcn.drop(columns=[column_name]).sum(axis=1)\n",
    "        \n",
    "        f = data_gcn[column_name]\n",
    "        \n",
    "        # 计算差异和方差\n",
    "        diff = f - f_s_array\n",
    "        alpha = 0.1   # 也可根据你的需求修改\n",
    "        c = 0.0       # 偏置项\n",
    "\n",
    "# 直接用 tanh 函数\n",
    "        e_f = np.tanh(alpha * diff + c)\n",
    "        \n",
    "        # 准备数据\n",
    "        ax_0 = [1] * expr_data.shape[0]\n",
    "        Logist_data = pd.DataFrame({\n",
    "            'a0': ax_0,\n",
    "            'f': f,\n",
    "            'f^2': f ** 2,\n",
    "            'e(f)': e_f,\n",
    "            'e(f)^2': e_f ** 2,\n",
    "            'f*e(f)': f * e_f\n",
    "        }, index=expr_data.index)\n",
    "        \n",
    "        # 标准化数据\n",
    "        scaler = StandardScaler()\n",
    "        Logist_data = pd.DataFrame(\n",
    "            scaler.fit_transform(Logist_data),\n",
    "            columns=Logist_data.columns,\n",
    "            index=Logist_data.index\n",
    "        )\n",
    "\n",
    "        y = label  # 假设label已定义，标签数据\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Logist_data, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        # 使用交叉验证评估模型\n",
    "        model_logi = LogisticRegression(max_iter=10000)  # 增加max_iter防止不收敛\n",
    "        cv_scores = cross_val_score(model_logi, X_train, y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "        print(f\"Cross-validation scores for {column_name}: {cv_scores}\")\n",
    "        print(f\"Mean cross-validation score: {np.mean(cv_scores)}\")\n",
    "\n",
    "        # 训练模型\n",
    "        model_logi.fit(X_train, y_train)\n",
    "        y_prob = model_logi.predict_proba(X_test)[:, 1]  # 获取阳性类别的概率\n",
    "\n",
    "        # 计算ACE（均方误差）\n",
    "        ace = mean_squared_error(y_test, y_prob)\n",
    "        \n",
    "        ace_values.append({\"特征\": column_name, \"ACE\": ace})\n",
    "        print(f\"特征 '{column_name}' 的 ACE: {ace}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing column {column_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 转换ACE值为DataFrame\n",
    "ace_df = pd.DataFrame(ace_values)\n",
    "print(ace_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40060c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ace_df_sorted = ace_df.sort_values(by=\"ACE\", ascending=True)\n",
    "ace_df_sorted\n",
    "# 将 ACE 值列表转换为 DataFrame\n",
    "ace_df_sorted_csv = pd.DataFrame(ace_df_sorted)\n",
    "\n",
    "# 保存 ACE 值 DataFrame 为 CSV 文件\n",
    "ace_csv_sortd_file = r'D:\\jupyter\\New\\result\\o_adata_tanh_ace_sorted_result.csv'\n",
    "ace_df_sorted_csv.to_csv(ace_csv_sortd_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing, model building, and evaluation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def plot_confusion_matrices(y_true, all_predictions, filename):\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    classes = np.unique(y_true)\n",
    "    \n",
    "    for idx, (clf_name, predictions) in enumerate(all_predictions.items(), 1):\n",
    "        plt.subplot(2, 2, idx)\n",
    "        \n",
    "        # 确保预测值和真实值的顺序一致\n",
    "        cm = confusion_matrix(y_true, predictions, labels=classes)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=classes, \n",
    "                    yticklabels=classes)\n",
    "        plt.title(f\"{clf_name} Confusion Matrix\")\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curves(y_true, classifiers_probas, filename):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for clf_name, probas in classifiers_probas.items():\n",
    "        if probas is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_true, probas)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{clf_name} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "class FixedStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.scaler.transform(X)\n",
    "\n",
    "def evaluate_classifier(classifier, data, label, features, max_features):\n",
    "    results = []\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    for num_features in range(1, max_features + 1):\n",
    "        selected_features = features[:num_features]\n",
    "        X = data[selected_features]\n",
    "        y = np.array(label)\n",
    "\n",
    "        fold_accuracy = []\n",
    "        fold_f1 = []\n",
    "        fold_auc = []\n",
    "\n",
    "        for train_index, test_index in cv.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', FixedStandardScaler()),\n",
    "                ('classifier', clone(classifier))\n",
    "            ])\n",
    "\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "\n",
    "            fold_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "            fold_f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "            if hasattr(classifier, 'predict_proba'):\n",
    "                y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "                try:\n",
    "                    fold_auc.append(roc_auc_score(y_test, y_pred_proba))\n",
    "                except:\n",
    "                    fold_auc.append(None)\n",
    "            else:\n",
    "                fold_auc.append(None)\n",
    "\n",
    "        results.append({\n",
    "            'NumFeatures': num_features, \n",
    "            'Accuracy': np.mean(fold_accuracy), \n",
    "            'F1Score': np.mean(fold_f1),\n",
    "            'AUC': np.mean([auc for auc in fold_auc if auc is not None]) if fold_auc else None\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def data_get_corss():\n",
    "    disese = pd.read_csv(r\"D:\\jupyter\\New\\dateset\\mRNA_LncRNA_Multiple_Sclerose.csv\")\n",
    "    health = pd.read_csv(r\"D:\\jupyter\\New\\dateset\\mRNA_LncRNA_Multiple_Sclerose_HC.csv\")\n",
    "    dat = disese\n",
    "    tes = health\n",
    "    return dat, tes\n",
    "\n",
    "def merge_data(dat, tes, r_res):\n",
    "    data = pd.merge(dat, tes)\n",
    "    df = data.iloc[:, 6:]\n",
    "    Y = list(r_res.keys())\n",
    "    df = df.transpose()\n",
    "    y = data.iloc[:, 2].values\n",
    "    df.columns = [y]\n",
    "    df = df.loc[:, Y]\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    #dat_ace, tes_ace = data_get_corss()\n",
    "    #data_merge = pd.merge(dat_ace, tes_ace)\n",
    "    #df_data = data_merge.iloc[:, 6:]\n",
    "    #df_data = df_data.transpose()\n",
    "    #dis_lab = [1]*dat_ace.iloc[0:,6:].shape[1]\n",
    "    #HC_lab = [0]*tes_ace.iloc[0:,6:].shape[1]\n",
    "    #df_lab = dis_lab+HC_lab\n",
    "\n",
    "    data = pd.read_csv(r'D:\\jupyter\\new_funtion\\data\\data1\\o_adata.csv')\n",
    "    df_data = data.drop(columns='Label')\n",
    "    df_lab = data['Label']\n",
    "\n",
    "    ace_df_sorted_csv = pd.read_csv(r\"D:\\jupyter\\New\\result\\o_adata_tanh_ace_sorted_result.csv\")\n",
    "    features = ace_df_sorted_csv['特征']\n",
    "    #y = dat_ace.iloc[:, 2].values\n",
    "    #df_data.columns = [y]\n",
    "    data_test = df_data\n",
    "    label = df_lab\n",
    "    label_array = np.array(label)\n",
    "\n",
    "    classifiers = {\n",
    "        'SVC': SVC(probability=True),\n",
    "        'XGBClassifier': XGBClassifier(),\n",
    "        'GaussianNB': GaussianNB(),\n",
    "        'DecisionTree': DecisionTreeClassifier()\n",
    "    }\n",
    "\n",
    "    feature_ranges = [50, 100, 200]\n",
    "\n",
    "    for max_features in feature_ranges:\n",
    "        print(f\"\\nEvaluating with maximum {max_features} features...\")\n",
    "        \n",
    "        results_summary = []\n",
    "        final_predictions = {}\n",
    "        final_probabilities = {}\n",
    "\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            print(f\"Evaluating {clf_name}...\")\n",
    "            result_df = evaluate_classifier(clf, data_test, label_array, features, max_features)\n",
    "\n",
    "            result_df_cleaned = result_df.dropna(subset=['F1Score'])\n",
    "            max_f1_index = result_df_cleaned['F1Score'].idxmax()\n",
    "            best_num_features = int(result_df_cleaned.loc[max_f1_index]['NumFeatures'])\n",
    "            best_f1_score = result_df_cleaned.loc[max_f1_index]['F1Score']\n",
    "            best_accuracy = result_df_cleaned.loc[max_f1_index]['Accuracy']\n",
    "            best_auc = result_df_cleaned.loc[max_f1_index]['AUC']\n",
    "\n",
    "            print(f\"{clf_name} - Best number of features: {best_num_features}, Best F1 Score: {best_f1_score}\")\n",
    "\n",
    "            X_selected = data_test[features[:best_num_features]]\n",
    "            cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "            \n",
    "            # 用于存储每个样本的预测结果和概率\n",
    "            predictions = np.zeros(len(label_array))\n",
    "            probabilities = np.zeros(len(label_array))\n",
    "            \n",
    "            # 记录每个样本被预测的次数\n",
    "            prediction_counts = np.zeros(len(label_array))\n",
    "\n",
    "            for train_index, test_index in cv.split(X_selected, label_array):\n",
    "                X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
    "                y_train = label_array[train_index]\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', FixedStandardScaler()),\n",
    "                    ('classifier', clone(clf))\n",
    "                ])\n",
    "\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                fold_predictions = pipeline.predict(X_test)\n",
    "                \n",
    "                # 累加预测结果\n",
    "                predictions[test_index] += fold_predictions\n",
    "                prediction_counts[test_index] += 1\n",
    "                \n",
    "                if hasattr(clf, 'predict_proba'):\n",
    "                    fold_probas = pipeline.predict_proba(X_test)[:, 1]\n",
    "                    probabilities[test_index] += fold_probas\n",
    "\n",
    "            # 计算最终预测结果（多数投票）\n",
    "            predictions = predictions / prediction_counts\n",
    "            predictions = (predictions > 0.5).astype(int)\n",
    "            \n",
    "            # 计算最终概率（平均）\n",
    "            if hasattr(clf, 'predict_proba'):\n",
    "                probabilities = probabilities / prediction_counts\n",
    "            \n",
    "            final_predictions[clf_name] = predictions\n",
    "            final_probabilities[clf_name] = probabilities\n",
    "\n",
    "            results_summary.append({\n",
    "                'Classifier': clf_name,\n",
    "                'Best_Num_Features': best_num_features,\n",
    "                'Best_F1_Score': best_f1_score,\n",
    "                'Best_Accuracy': best_accuracy,\n",
    "                'Best_AUC': best_auc\n",
    "            })\n",
    "\n",
    "        # 保存结果\n",
    "        confusion_matrix_path = f'D:\\\\jupyter\\\\New\\\\result\\\\cm\\\\{max_features}-o_adata_tanh_confusion_matrices.png'\n",
    "        roc_curve_path = f'D:\\\\jupyter\\\\New\\\\result\\\\roc\\\\{max_features}-o_adata_tanh_roc_curves.png'\n",
    "        \n",
    "        plot_confusion_matrices(label_array, final_predictions, confusion_matrix_path)\n",
    "        plot_roc_curves(label_array, final_probabilities, roc_curve_path)\n",
    "\n",
    "        best_result_df = pd.DataFrame(results_summary)\n",
    "        result_csv_path = f'D:\\\\jupyter\\\\New\\\\result\\\\f1-score\\\\{max_features}-o_adata_tanh_best_classification_results.csv'\n",
    "        best_result_df.to_csv(result_csv_path, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
